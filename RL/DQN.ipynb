{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "Episode 1 with Reward : 507.17135317144607 at epsilon 0.38959999999998973 in steps 2948\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "target parameters changed\n",
      "Episode 2 with Reward : 207.42972852468728 at epsilon 0.9001999999999335 in steps 2626\n",
      "Episode 3 with Reward : 44.81624296650356 at epsilon 0.9001999999999335 in steps 157\n",
      "target parameters changed\n",
      "Episode 4 with Reward : 107.30393254812272 at epsilon 0.9001999999999335 in steps 303\n",
      "Episode 5 with Reward : 64.0993149363967 at epsilon 0.9001999999999335 in steps 199\n",
      "target parameters changed\n",
      "Episode 6 with Reward : 92.47730773469517 at epsilon 0.9001999999999335 in steps 379\n",
      "Episode 7 with Reward : 65.96446937314838 at epsilon 0.9001999999999335 in steps 216\n",
      "target parameters changed\n",
      "Episode 8 with Reward : 66.93873421556097 at epsilon 0.9001999999999335 in steps 191\n",
      "Episode 9 with Reward : 69.36049366104744 at epsilon 0.9001999999999335 in steps 195\n",
      "Episode 10 with Reward : 92.6907847491528 at epsilon 0.9001999999999335 in steps 243\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "예제 7-3: Mountain Car에 적용한 DQN 예제\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self,learning_rate, gamma, n_features, n_actions, epsilon, parameter_changing_pointer, memory_size):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = 100\n",
    "        self.experience_counter = 0\n",
    "        self.experience_limit = memory_size\n",
    "        self.replace_target_pointer = parameter_changing_pointer\n",
    "        self.learning_counter = 0\n",
    "        self.memory = np.zeros([self.experience_limit,self.n_features*2+2])  \n",
    "\n",
    "        self.build_networks()\n",
    "        p_params = tf.get_collection('primary_network_parameters')\n",
    "        t_params = tf.get_collection('target_network_parameters')\n",
    "        self.replacing_target_parameters = [tf.assign(t,p) for t,p in\n",
    "\t\t\t\t\t\t zip(t_params,p_params)]\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#------------------------------------------------------\n",
    "\n",
    "    def build_networks(self):\n",
    "        hidden_units = 10\n",
    "        # Primary Network\n",
    "        self.s = tf.placeholder(tf.float32,[None,self.n_features])\n",
    "        self.qtarget = tf.placeholder(tf.float32,[None,self.n_actions])\n",
    "\n",
    "        with tf.variable_scope('primary_network'):\n",
    "            c = ['primary_network_parameters', \n",
    "\t\t\t\t\ttf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            # first layer\n",
    "            with tf.variable_scope('layer1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, hidden_units],\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                b1 = tf.get_variable('b1', [1, hidden_units],\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                l1 = tf.nn.relu(tf.matmul(self.s, w1) + b1)\n",
    "\n",
    "            # second layer\n",
    "            with tf.variable_scope('layer2'):\n",
    "                w2 = tf.get_variable('w2', [hidden_units, self.n_actions],\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions],\n",
    "\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                self.qeval = tf.matmul(l1, w2) + b2\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "                self.loss = tf.reduce_mean(tf.squared_difference(\n",
    "\t\t\t\t\t\tself.qtarget, self.qeval))\n",
    "        with tf.variable_scope('optimiser'):\n",
    "                self.train = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "        # Target Network\n",
    "        self.st = tf.placeholder(tf.float32,[None,self.n_features])\n",
    "\n",
    "        with tf.variable_scope('target_network'):\n",
    "            c = ['target_network_parameters', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "            # first layer\n",
    "            with tf.variable_scope('layer1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, hidden_units],\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                b1 = tf.get_variable('b1', [1, hidden_units],\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                l1 = tf.nn.relu(tf.matmul(self.st, w1) + b1)\n",
    "\n",
    "            # second layer\n",
    "            with tf.variable_scope('layer2'):\n",
    "                w2 = tf.get_variable('w2', [hidden_units, self.n_actions],\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions],\n",
    "\t\t\t\tinitializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     dtype=tf.float32,collections=c)\n",
    "\n",
    "                self.qt = tf.matmul(l1, w2) + b2\n",
    "\n",
    "#-----------------------------------------------\n",
    "\n",
    "    def target_params_replaced(self):\n",
    "        self.sess.run(self.replacing_target_parameters)\n",
    "\n",
    "    def store_experience(self,obs,a,r,obs_):\n",
    "        index = self.experience_counter % self.experience_limit\n",
    "        self.memory[index,:] = np.hstack((obs,[a,r],obs_))\n",
    "        self.experience_counter+=1\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "    def fit(self):\n",
    "        # sample batch memory from all memory\n",
    "        if self.experience_counter < self.experience_limit:\n",
    "            indices = np.random.choice(self.experience_counter, size=self.batch_size)\n",
    "        else:\n",
    "            indices = np.random.choice(self.experience_limit, size=self.batch_size)\n",
    "\n",
    "        batch = self.memory[indices,:]\n",
    "        qt,qeval = self.sess.run([self.qt,self.qeval],\n",
    "\tfeed_dict={self.st:batch[:,-self.n_features:],self.s:batch[:,:self.n_features]})\n",
    "\n",
    "        qtarget = qeval.copy()    \n",
    "        batch_indices = np.arange(self.batch_size, dtype=np.int32)\n",
    "        actions = self.memory[indices,self.n_features].astype(int)\n",
    "        rewards = self.memory[indices,self.n_features+1]\n",
    "        qtarget[batch_indices,actions] = rewards + self.gamma * np.max(qt,axis=1)\n",
    "\n",
    "        _ = self.sess.run(self.train,feed_dict = {self.s:batch[:,:self.n_features],\n",
    "\t\t\t\t\t\t\tself.qtarget:qtarget})\n",
    "        if self.epsilon < 0.9:\n",
    "            self.epsilon += 0.0002\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "        if self.learning_counter % self.replace_target_pointer == 0:\n",
    "            self.target_params_replaced()\n",
    "            print(\"target parameters changed\")\n",
    "        self.learning_counter += 1\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "    def epsilon_greedy(self,obs):\n",
    "        #epsilon greedy implementation to choose action\n",
    "        if np.random.uniform(low=0,high=1) < self.epsilon:\n",
    "            return np.argmax(self.sess.run(self.qeval,\n",
    "\t\t\t\t\tfeed_dict={self.s:obs[np.newaxis,:]}))\n",
    "        else:\n",
    "            return np.random.choice(self.n_actions)\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env = env.unwrapped\n",
    "    dqn = DQN(learning_rate=0.001, gamma=0.9, n_features=env.observation_space.shape[0],\n",
    "              n_actions=env.action_space.n, epsilon=0.0, parameter_changing_pointer=500,\n",
    "              memory_size=5000)\n",
    "\n",
    "    episodes = 10\n",
    "    total_steps = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        steps = 0\t\t\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            env.render()\n",
    "            action = dqn.epsilon_greedy(obs)\n",
    "            obs_,reward,terminate,_ = env.step(action)\n",
    "            reward = abs(obs_[0]+0.5)\n",
    "            dqn.store_experience(obs,action,reward,obs_)\n",
    "            if total_steps > 1000:\n",
    "                dqn.fit()\n",
    "            episode_reward+=reward\n",
    "            if terminate:\n",
    "                break\n",
    "            obs = obs_\n",
    "            total_steps+=1\n",
    "            steps+=1\n",
    "        print(\"Episode {} with Reward : {} at epsilon {} in steps {}\".\n",
    "\t\t\tformat(episode+1,episode_reward,dqn.epsilon,steps))\n",
    "\n",
    "    while True:\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
