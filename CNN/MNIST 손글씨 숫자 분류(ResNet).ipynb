{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-71aa3f4f1bd7>:89: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch: 1 cost= 2.061093, accuracy= 0.500000\n",
      "Epoch: 2 cost= 1.837618, accuracy= 0.590000\n",
      "Epoch: 3 cost= 1.773470, accuracy= 0.650000\n",
      "Epoch: 4 cost= 1.764926, accuracy= 0.700000\n",
      "Epoch: 5 cost= 1.695737, accuracy= 0.820000\n",
      "Epoch: 6 cost= 1.673293, accuracy= 0.800000\n",
      "Epoch: 7 cost= 1.668315, accuracy= 0.760000\n",
      "Epoch: 8 cost= 1.664733, accuracy= 0.770000\n",
      "Epoch: 9 cost= 1.663474, accuracy= 0.730000\n",
      "Epoch: 10 cost= 1.661391, accuracy= 0.780000\n",
      "misclassification error(tr): 0.20192726612091105\n",
      "misclassification error(ts): 0.20340000152587911\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple \n",
    "##from libs.connections import  conv2d, linear \n",
    "\n",
    "def linear(x, n_units, scope=None, stddev=0.02,\n",
    "           activation=lambda x: x):\n",
    "    \"\"\"Fully-connected network.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor\n",
    "        Input tensor to the network.\n",
    "    n_units : int\n",
    "        Number of units to connect to.\n",
    "    scope : str, optional\n",
    "        Variable scope to use.\n",
    "    stddev : float, optional\n",
    "        Initialization's standard deviation.\n",
    "    activation : arguments, optional\n",
    "        Function which applies a nonlinearity\n",
    "    Returns\n",
    "    -------\n",
    "    x : Tensor\n",
    "        Fully-connected output.\n",
    "    \"\"\"\n",
    "    shape = x.get_shape().as_list()\n",
    "\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[1], n_units], tf.float32,\n",
    "                                 tf.random_normal_initializer(stddev=stddev))\n",
    "        return activation(tf.matmul(x, matrix))\n",
    "def conv2d(x, n_filters,\n",
    "           k_h=5, k_w=5,\n",
    "           stride_h=2, stride_w=2,\n",
    "           stddev=0.02,\n",
    "           activation=None,\n",
    "           bias=True,\n",
    "           padding='SAME',\n",
    "           name=\"Conv2D\"):\n",
    "    \"\"\"2D Convolution with options for kernel size, stride, and init deviation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor\n",
    "        Input tensor to convolve.\n",
    "    n_filters : int\n",
    "        Number of filters to apply.\n",
    "    k_h : int, optional\n",
    "        Kernel height.\n",
    "    k_w : int, optional\n",
    "        Kernel width.\n",
    "    stride_h : int, optional\n",
    "        Stride in rows.\n",
    "    stride_w : int, optional\n",
    "        Stride in cols.\n",
    "    stddev : float, optional\n",
    "        Initialization's standard deviation.\n",
    "    activation : arguments, optional\n",
    "        Function which applies a nonlinearity\n",
    "    padding : str, optional\n",
    "        'SAME' or 'VALID'\n",
    "    name : str, optional\n",
    "        Variable scope to use.\n",
    "    Returns\n",
    "    -------\n",
    "    x : Tensor\n",
    "        Convolved input.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable(\n",
    "            'w', [k_h, k_w, x.get_shape()[-1], n_filters],\n",
    "            initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(\n",
    "            x, w, strides=[1, stride_h, stride_w, 1], padding=padding)\n",
    "        if bias:\n",
    "            b = tf.get_variable(\n",
    "                'b', [n_filters],\n",
    "                initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "            conv = tf.nn.bias_add(conv, b)\n",
    "        if activation:\n",
    "            conv = activation(conv)\n",
    "        return conv\n",
    "\n",
    "# 그래프 리셋\n",
    "tf.reset_default_graph()\n",
    "# 재현성을 위해 시드 지정\n",
    "tf.set_random_seed(1)\n",
    "# 자료 입력\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "learning_rate=0.0001; epochs=10; batch_size=100\n",
    "X=tf.placeholder(tf.float32,[None,784]) \n",
    "X_img=tf.reshape(X,[-1,28,28,1]) \n",
    "Y=tf.placeholder(tf.float32,[None,10]) \n",
    "# ResNet 블록 구조(bottleneck 구조) \n",
    "LayerBlock = namedtuple('LayerBlock', ['num_repeats', 'num_filters', 'bottleneck_size'])\n",
    "blocks = [LayerBlock(3, 128, 32),LayerBlock(3, 256, 64),LayerBlock(3, 512, 128),\n",
    "          LayerBlock(3, 1024, 256)]\n",
    "# 채널수 64의 합성곱 출력을 만들고 다운샘플링\n",
    "net = conv2d(X_img, 64, k_h=7, k_w=7, name='conv1', activation=tf.nn.relu)\n",
    "net = tf.nn.max_pool(net, [1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "# ResNet 블록구조의 입력 생성 \n",
    "net = conv2d(net, blocks[0].num_filters, k_h=1, k_w=1, stride_h=1, stride_w=1, \n",
    "      padding='VALID', name='conv2')\n",
    "# ResNet 블록 반복 \n",
    "for block_i, block in enumerate(blocks):\n",
    "    for repeat_i in range(block.num_repeats):\n",
    "        name = 'block_%d/repeat_%d' % (block_i, repeat_i)\n",
    "        conv1 = conv2d(net, block.bottleneck_size, k_h=1, k_w=1, \n",
    "                         padding='VALID', stride_h=1, stride_w=1, \n",
    "                         activation=tf.nn.relu,name=name + '/conv_in')                      \n",
    "        conv2 = conv2d(conv1, block.bottleneck_size, k_h=3, k_w=3,\n",
    "                         padding='SAME', stride_h=1, stride_w=1,\n",
    "                         activation=tf.nn.relu,name=name + '/conv_bottleneck')\n",
    "        conv3 = conv2d(conv2, block.num_filters, k_h=1, k_w=1,\n",
    "                         padding='VALID', stride_h=1, stride_w=1,\n",
    "                         activation=tf.nn.relu, name=name + '/conv_out')\n",
    "\n",
    "        net = conv3 + net\n",
    "    try:\n",
    "# upscale to the next block size\n",
    "        next_block = blocks[block_i + 1]\n",
    "        net = conv2d(net, next_block.num_filters, k_h=1, k_w=1,\n",
    "                         padding='SAME', stride_h=1, stride_w=1, bias=False,\n",
    "                         name='block_%d/conv_upscale' % block_i)\n",
    "    except IndexError:\n",
    "        pass\n",
    "# 평균 풀링을 이용하여 블록 구조의 최종 출력의 차원 변환\n",
    "net = tf.nn.avg_pool(net, ksize=[1, net.get_shape().as_list()[1],net.get_shape().as_list()[2], 1],\n",
    "                     strides=[1, 1, 1, 1], padding='VALID')\n",
    "#ResNet 블록 구조의 최종 출력을 1D로 변환\n",
    "Flat=tf.reshape(net,[-1, net.get_shape().as_list()[1] *net.get_shape().as_list()[2] \n",
    "               *net.get_shape().as_list()[3]])\n",
    "# 최종 출력을 위해 소프트맥스함수 지정\n",
    "Y_pred =linear(Flat, 10, activation=tf.nn.softmax)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Y_pred, labels=Y))\n",
    "optim=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_predict=tf.equal(tf.argmax(Y_pred,1), tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "sess=tf.Session(); sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        feed_dict={X:batch_xs, Y:batch_ys}\n",
    "        sess.run(optim, feed_dict=feed_dict)\n",
    "        ccost=sess.run(cost, feed_dict=feed_dict)\n",
    "        avg_cost+=ccost/total_batch\n",
    "        acc=sess.run(accuracy, feed_dict=feed_dict)\n",
    "    print('Epoch: %d' %(epoch+1),'cost= %f, accuracy= %f' %(avg_cost, acc))\n",
    "# 훈련 데이터, 검정 데이터의 오분류율\n",
    "acc_tr=0; acc_ts=0\n",
    "for ii in range(100): #메모리 문제를 피하기 위해 자료를 100개로 분할\n",
    "    xr,yr=mnist.train.next_batch(550)\n",
    "    acc_tr= acc_tr+ 0.01*sess.run(accuracy, feed_dict={X:xr, Y:yr}) \n",
    "    xt,yt=mnist.test.next_batch(100)\n",
    "    acc_ts= acc_ts+ 0.01*sess.run(accuracy, feed_dict={X:xt, Y:yt})    \n",
    "print('misclassification error(tr):', 1-acc_tr)\n",
    "print('misclassification error(ts):', 1-acc_ts)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
