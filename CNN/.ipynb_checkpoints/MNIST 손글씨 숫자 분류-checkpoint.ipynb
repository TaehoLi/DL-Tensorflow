{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-425acb64e6a7>:40: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-425acb64e6a7>:55: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "Epoch: 1 cost= 2.299615, accuracy= 0.190000\n",
      "Epoch: 2 cost= 2.293342, accuracy= 0.330000\n",
      "Epoch: 3 cost= 2.284247, accuracy= 0.310000\n",
      "Epoch: 4 cost= 2.267799, accuracy= 0.330000\n",
      "Epoch: 5 cost= 2.232848, accuracy= 0.400000\n",
      "Epoch: 6 cost= 2.171326, accuracy= 0.390000\n",
      "Epoch: 7 cost= 2.090290, accuracy= 0.480000\n",
      "Epoch: 8 cost= 2.018894, accuracy= 0.580000\n",
      "Epoch: 9 cost= 1.953998, accuracy= 0.450000\n",
      "Epoch: 10 cost= 1.893993, accuracy= 0.750000\n",
      "Epoch: 11 cost= 1.841637, accuracy= 0.760000\n",
      "Epoch: 12 cost= 1.806366, accuracy= 0.710000\n",
      "Epoch: 13 cost= 1.783459, accuracy= 0.680000\n",
      "Epoch: 14 cost= 1.767808, accuracy= 0.740000\n",
      "Epoch: 15 cost= 1.756456, accuracy= 0.780000\n",
      "Epoch: 16 cost= 1.747799, accuracy= 0.710000\n",
      "Epoch: 17 cost= 1.740762, accuracy= 0.800000\n",
      "Epoch: 18 cost= 1.734535, accuracy= 0.770000\n",
      "Epoch: 19 cost= 1.726912, accuracy= 0.760000\n",
      "Epoch: 20 cost= 1.704181, accuracy= 0.780000\n",
      "Epoch: 21 cost= 1.686768, accuracy= 0.830000\n",
      "Epoch: 22 cost= 1.677294, accuracy= 0.870000\n",
      "Epoch: 23 cost= 1.669708, accuracy= 0.820000\n",
      "Epoch: 24 cost= 1.663610, accuracy= 0.780000\n",
      "Epoch: 25 cost= 1.658372, accuracy= 0.820000\n",
      "Epoch: 26 cost= 1.653989, accuracy= 0.860000\n",
      "Epoch: 27 cost= 1.650089, accuracy= 0.790000\n",
      "Epoch: 28 cost= 1.646712, accuracy= 0.890000\n",
      "Epoch: 29 cost= 1.643757, accuracy= 0.820000\n",
      "Epoch: 30 cost= 1.641085, accuracy= 0.830000\n",
      "Epoch: 31 cost= 1.638633, accuracy= 0.840000\n",
      "Epoch: 32 cost= 1.636439, accuracy= 0.870000\n",
      "Epoch: 33 cost= 1.634403, accuracy= 0.880000\n",
      "Epoch: 34 cost= 1.632541, accuracy= 0.800000\n",
      "Epoch: 35 cost= 1.630810, accuracy= 0.850000\n",
      "Epoch: 36 cost= 1.629190, accuracy= 0.830000\n",
      "Epoch: 37 cost= 1.627672, accuracy= 0.850000\n",
      "Epoch: 38 cost= 1.626236, accuracy= 0.790000\n",
      "Epoch: 39 cost= 1.624903, accuracy= 0.810000\n",
      "Epoch: 40 cost= 1.623645, accuracy= 0.830000\n",
      "Epoch: 41 cost= 1.622406, accuracy= 0.860000\n",
      "Epoch: 42 cost= 1.621270, accuracy= 0.830000\n",
      "Epoch: 43 cost= 1.620198, accuracy= 0.760000\n",
      "Epoch: 44 cost= 1.619121, accuracy= 0.800000\n",
      "Epoch: 45 cost= 1.618139, accuracy= 0.890000\n",
      "Epoch: 46 cost= 1.617194, accuracy= 0.870000\n",
      "Epoch: 47 cost= 1.616254, accuracy= 0.870000\n",
      "Epoch: 48 cost= 1.615363, accuracy= 0.900000\n",
      "Epoch: 49 cost= 1.614466, accuracy= 0.870000\n",
      "Epoch: 50 cost= 1.613617, accuracy= 0.850000\n",
      "Epoch: 51 cost= 1.612720, accuracy= 0.870000\n",
      "Epoch: 52 cost= 1.611898, accuracy= 0.820000\n",
      "Epoch: 53 cost= 1.611003, accuracy= 0.870000\n",
      "Epoch: 54 cost= 1.610057, accuracy= 0.800000\n",
      "Epoch: 55 cost= 1.608899, accuracy= 0.920000\n",
      "Epoch: 56 cost= 1.607305, accuracy= 0.840000\n",
      "Epoch: 57 cost= 1.603737, accuracy= 0.880000\n",
      "Epoch: 58 cost= 1.585198, accuracy= 0.900000\n",
      "Epoch: 59 cost= 1.564108, accuracy= 0.900000\n",
      "Epoch: 60 cost= 1.558899, accuracy= 0.930000\n",
      "Epoch: 61 cost= 1.555050, accuracy= 0.920000\n",
      "Epoch: 62 cost= 1.551787, accuracy= 0.940000\n",
      "Epoch: 63 cost= 1.548915, accuracy= 0.950000\n",
      "Epoch: 64 cost= 1.546505, accuracy= 0.950000\n",
      "Epoch: 65 cost= 1.544290, accuracy= 0.940000\n",
      "Epoch: 66 cost= 1.542231, accuracy= 0.980000\n",
      "Epoch: 67 cost= 1.540376, accuracy= 0.930000\n",
      "Epoch: 68 cost= 1.538738, accuracy= 0.920000\n",
      "Epoch: 69 cost= 1.537233, accuracy= 0.980000\n",
      "Epoch: 70 cost= 1.535747, accuracy= 0.930000\n",
      "Epoch: 71 cost= 1.534394, accuracy= 0.960000\n",
      "Epoch: 72 cost= 1.533183, accuracy= 0.930000\n",
      "Epoch: 73 cost= 1.532024, accuracy= 0.930000\n",
      "Epoch: 74 cost= 1.530900, accuracy= 0.950000\n",
      "Epoch: 75 cost= 1.529900, accuracy= 0.980000\n",
      "Epoch: 76 cost= 1.528898, accuracy= 0.970000\n",
      "Epoch: 77 cost= 1.527969, accuracy= 0.970000\n",
      "Epoch: 78 cost= 1.527042, accuracy= 0.930000\n",
      "Epoch: 79 cost= 1.526274, accuracy= 0.900000\n",
      "Epoch: 80 cost= 1.525421, accuracy= 0.970000\n",
      "Epoch: 81 cost= 1.524661, accuracy= 0.910000\n",
      "Epoch: 82 cost= 1.523943, accuracy= 0.970000\n",
      "Epoch: 83 cost= 1.523186, accuracy= 0.950000\n",
      "Epoch: 84 cost= 1.522500, accuracy= 0.940000\n",
      "Epoch: 85 cost= 1.521879, accuracy= 0.940000\n",
      "Epoch: 86 cost= 1.521200, accuracy= 0.940000\n",
      "Epoch: 87 cost= 1.520584, accuracy= 0.920000\n",
      "Epoch: 88 cost= 1.519986, accuracy= 0.940000\n",
      "Epoch: 89 cost= 1.519345, accuracy= 0.920000\n",
      "Epoch: 90 cost= 1.518799, accuracy= 0.950000\n",
      "Epoch: 91 cost= 1.518261, accuracy= 0.930000\n",
      "Epoch: 92 cost= 1.517715, accuracy= 0.980000\n",
      "Epoch: 93 cost= 1.517219, accuracy= 0.950000\n",
      "Epoch: 94 cost= 1.516720, accuracy= 0.970000\n",
      "Epoch: 95 cost= 1.516170, accuracy= 0.960000\n",
      "Epoch: 96 cost= 1.515750, accuracy= 0.960000\n",
      "Epoch: 97 cost= 1.515216, accuracy= 0.990000\n",
      "Epoch: 98 cost= 1.514804, accuracy= 0.960000\n",
      "Epoch: 99 cost= 1.514315, accuracy= 0.990000\n",
      "Epoch: 100 cost= 1.513948, accuracy= 1.000000\n",
      "misclassification error(tr): 0.04130911827087402\n",
      "misclassification error(ts): 0.038999974727630615\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from libs.connections import linear \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def linear(x, n_units, scope=None, stddev=0.02,\n",
    "           activation=lambda x: x):\n",
    "    \"\"\"Fully-connected network.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor\n",
    "        Input tensor to the network.\n",
    "    n_units : int\n",
    "        Number of units to connect to.\n",
    "    scope : str, optional\n",
    "        Variable scope to use.\n",
    "    stddev : float, optional\n",
    "        Initialization's standard deviation.\n",
    "    activation : arguments, optional\n",
    "        Function which applies a nonlinearity\n",
    "    Returns\n",
    "    -------\n",
    "    x : Tensor\n",
    "        Fully-connected output.\n",
    "    \"\"\"\n",
    "    shape = x.get_shape().as_list()\n",
    "\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[1], n_units], tf.float32,\n",
    "                                 tf.random_normal_initializer(stddev=stddev))\n",
    "        return activation(tf.matmul(x, matrix))\n",
    "\n",
    "\n",
    "# 그래프 리셋\n",
    "tf.reset_default_graph()\n",
    "# 재현성을 위해 시드 지정\n",
    "tf.set_random_seed(1)\n",
    "# 자료 입력\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "# 훈련 데이터 이미지\n",
    "f,axes =plt.subplots(figsize=(7,7), nrows=3, ncols=4, sharey=True, sharex=True)\n",
    "for ii in range(12):\n",
    "    plt.subplot(3,4,ii+1)\n",
    "    plt.imshow(mnist.train.images[ii].reshape(28,28),cmap='Greys_r')\n",
    "\n",
    "learning_rate=0.001; epochs=100; batch_size=100\n",
    "X=tf.placeholder(tf.float32,[None,784]) \n",
    "X_img=tf.reshape(X,[-1,28,28,1]) \n",
    "Y=tf.placeholder(tf.float32,[None,10]) \n",
    "# 첫 번째 합성곱층에 사용하는 필터 크기와 개수, 보폭 지정\n",
    "K1=tf.Variable(tf.random_normal([5,5,1,20],stddev=0.01))\n",
    "a1=tf.nn.conv2d(X_img, K1, strides=[1,1,1,1], padding='VALID')\n",
    "# 배치정규화 \n",
    "a1=tf.layers.batch_normalization(a1, training=True)\n",
    "# 첫 번째 합성곱층의 활성화함수 지정 \n",
    "a1=tf.nn.relu(a1) \n",
    "# 첫 번째 풀링층에 사용하는 풀링의 종류와 크기, 보폭 지정\n",
    "h1=tf.nn.max_pool(a1,ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "# 두 번째 합성곱층에 사용하는 필터 크기와 개수, 보폭 지정\n",
    "K2=tf.Variable(tf.random_normal([5,5,20,50],stddev=0.01))\n",
    "a2=tf.nn.conv2d(h1, K2, strides=[1,1,1,1], padding='VALID')\n",
    "# 배치정규화 \n",
    "a2=tf.layers.batch_normalization(a2, training=True)\n",
    "# 두 번째 합성곱층의 활성화함수 지정 \n",
    "a2=tf.nn.relu(a2) \n",
    "# 두 번째 풀링층에 사용하는 풀링의 종류와 크기, 보폭 지정\n",
    "h2=tf.nn.max_pool(a1,ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "#두 번째 풀링층의 출력을 1D로 변환\n",
    "Flat=tf.reshape(h2,[-1,np.prod(h2.get_shape().as_list()[1:4])])\n",
    "#완전 연결 신경망의 은닉층의 구조 지정\n",
    "W1=tf.get_variable(\"W1\",shape=[np.prod(h1.get_shape().as_list()[1:4]),50],initializer\n",
    "    =tf.contrib.layers.xavier_initializer())\n",
    "b1=tf.Variable(tf.random_normal([50]))\n",
    "L1=tf.matmul(Flat, W1)+b1\n",
    "# 최종 출력을 위해 소프트맥스함수 지정\n",
    "Y_pred =linear(L1, 10, activation=tf.nn.softmax)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Y_pred,labels=Y)) \n",
    "optim=tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "correct_predict=tf.equal(tf.argmax(Y_pred,1), tf.argmax(Y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "sess=tf.Session(); sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(epochs):\n",
    "    avg_cost=0\n",
    "    total_batch=int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        feed_dict={X:batch_xs, Y:batch_ys}\n",
    "        sess.run(optim, feed_dict=feed_dict)\n",
    "        ccost=sess.run(cost, feed_dict=feed_dict)\n",
    "        avg_cost+=ccost/total_batch\n",
    "        acc=sess.run(accuracy, feed_dict=feed_dict)\n",
    "    print('Epoch: %d' %(epoch+1),'cost= %f, accuracy= %f' %(avg_cost, acc))\n",
    "# 훈련 데이터, 검정 데이터의 오분류율\n",
    "print('misclassification error(tr):', 1-sess.run(accuracy, feed_dict= \n",
    "               {X:mnist.train.images, Y:mnist.train.labels}))\n",
    "print('misclassification error(ts):', 1-sess.run(accuracy, feed_dict=         \n",
    "               {X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
