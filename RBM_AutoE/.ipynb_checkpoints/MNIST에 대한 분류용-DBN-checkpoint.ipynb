{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-b370857632af>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch: 0001\n",
      "Epoch: 0002\n",
      "Epoch: 0003\n",
      "Epoch: 0004\n",
      "Epoch: 0005\n",
      "Epoch: 0006\n",
      "Epoch: 0007\n",
      "Epoch: 0008\n",
      "Epoch: 0009\n",
      "Epoch: 0010\n",
      "Epoch: 0011\n",
      "Epoch: 0012\n",
      "Epoch: 0013\n",
      "Epoch: 0014\n",
      "Epoch: 0015\n",
      "Epoch: 0016\n",
      "Epoch: 0017\n",
      "Epoch: 0018\n",
      "Epoch: 0019\n",
      "Epoch: 0020\n",
      "Epoch: 0021\n",
      "Epoch: 0022\n",
      "Epoch: 0023\n",
      "Epoch: 0024\n",
      "Epoch: 0025\n",
      "Epoch: 0026\n",
      "Epoch: 0027\n",
      "Epoch: 0028\n",
      "Epoch: 0029\n",
      "Epoch: 0030\n",
      "Epoch: 0031\n",
      "Epoch: 0032\n",
      "Epoch: 0033\n",
      "Epoch: 0034\n",
      "Epoch: 0035\n",
      "Epoch: 0036\n",
      "Epoch: 0037\n",
      "Epoch: 0038\n",
      "Epoch: 0039\n",
      "Epoch: 0040\n",
      "Epoch: 0041\n",
      "Epoch: 0042\n",
      "Epoch: 0043\n",
      "Epoch: 0044\n",
      "Epoch: 0045\n",
      "Epoch: 0046\n",
      "Epoch: 0047\n",
      "Epoch: 0048\n",
      "Epoch: 0049\n",
      "Epoch: 0050\n",
      "Epoch: 0051\n",
      "Epoch: 0052\n",
      "Epoch: 0053\n",
      "Epoch: 0054\n",
      "Epoch: 0055\n",
      "Epoch: 0056\n",
      "Epoch: 0057\n",
      "Epoch: 0058\n",
      "Epoch: 0059\n",
      "Epoch: 0060\n",
      "Epoch: 0061\n",
      "Epoch: 0062\n",
      "Epoch: 0063\n",
      "Epoch: 0064\n",
      "Epoch: 0065\n",
      "Epoch: 0066\n",
      "Epoch: 0067\n",
      "Epoch: 0068\n",
      "Epoch: 0069\n",
      "Epoch: 0070\n",
      "Epoch: 0071\n",
      "Epoch: 0072\n",
      "Epoch: 0073\n",
      "Epoch: 0074\n",
      "Epoch: 0075\n",
      "Epoch: 0076\n",
      "Epoch: 0077\n",
      "Epoch: 0078\n",
      "Epoch: 0079\n",
      "Epoch: 0080\n",
      "Epoch: 0081\n",
      "Epoch: 0082\n",
      "Epoch: 0083\n",
      "Epoch: 0084\n",
      "Epoch: 0085\n",
      "Epoch: 0086\n",
      "Epoch: 0087\n",
      "Epoch: 0088\n",
      "Epoch: 0089\n",
      "Epoch: 0090\n",
      "Epoch: 0091\n",
      "Epoch: 0092\n",
      "Epoch: 0093\n",
      "Epoch: 0094\n",
      "Epoch: 0095\n",
      "Epoch: 0096\n",
      "Epoch: 0097\n",
      "Epoch: 0098\n",
      "Epoch: 0099\n",
      "Epoch: 0100\n",
      "Epoch: 0101\n",
      "Epoch: 0102\n",
      "Epoch: 0103\n",
      "Epoch: 0104\n",
      "Epoch: 0105\n",
      "Epoch: 0106\n",
      "Epoch: 0107\n",
      "Epoch: 0108\n",
      "Epoch: 0109\n",
      "Epoch: 0110\n",
      "Epoch: 0111\n",
      "Epoch: 0112\n",
      "Epoch: 0113\n",
      "Epoch: 0114\n",
      "Epoch: 0115\n",
      "Epoch: 0116\n",
      "Epoch: 0117\n",
      "Epoch: 0118\n",
      "Epoch: 0119\n",
      "Epoch: 0120\n",
      "Epoch: 0121\n",
      "Epoch: 0122\n",
      "Epoch: 0123\n",
      "Epoch: 0124\n",
      "Epoch: 0125\n",
      "Epoch: 0126\n",
      "Epoch: 0127\n",
      "Epoch: 0128\n",
      "Epoch: 0129\n",
      "Epoch: 0130\n",
      "Epoch: 0131\n",
      "Epoch: 0132\n",
      "Epoch: 0133\n",
      "Epoch: 0134\n",
      "Epoch: 0135\n",
      "Epoch: 0136\n",
      "Epoch: 0137\n",
      "Epoch: 0138\n",
      "Epoch: 0139\n",
      "Epoch: 0140\n",
      "Epoch: 0141\n",
      "Epoch: 0142\n",
      "Epoch: 0143\n",
      "Epoch: 0144\n",
      "Epoch: 0145\n",
      "Epoch: 0146\n",
      "Epoch: 0147\n",
      "Epoch: 0148\n",
      "Epoch: 0149\n",
      "Epoch: 0150\n",
      "Epoch: 0151\n",
      "Epoch: 0152\n",
      "Epoch: 0153\n",
      "Epoch: 0154\n",
      "Epoch: 0155\n",
      "Epoch: 0156\n",
      "Epoch: 0157\n",
      "Epoch: 0158\n",
      "Epoch: 0159\n",
      "Epoch: 0160\n",
      "Epoch: 0161\n",
      "Epoch: 0162\n",
      "Epoch: 0163\n",
      "Epoch: 0164\n",
      "Epoch: 0165\n",
      "Epoch: 0166\n",
      "Epoch: 0167\n",
      "Epoch: 0168\n",
      "Epoch: 0169\n",
      "Epoch: 0170\n",
      "Epoch: 0171\n",
      "Epoch: 0172\n",
      "Epoch: 0173\n",
      "Epoch: 0174\n",
      "Epoch: 0175\n",
      "Epoch: 0176\n",
      "Epoch: 0177\n",
      "Epoch: 0178\n",
      "Epoch: 0179\n",
      "Epoch: 0180\n",
      "Epoch: 0181\n",
      "Epoch: 0182\n",
      "Epoch: 0183\n",
      "Epoch: 0184\n",
      "Epoch: 0185\n",
      "Epoch: 0186\n",
      "Epoch: 0187\n",
      "Epoch: 0188\n",
      "Epoch: 0189\n",
      "Epoch: 0190\n",
      "Epoch: 0191\n",
      "Epoch: 0192\n",
      "Epoch: 0193\n",
      "Epoch: 0194\n",
      "Epoch: 0195\n",
      "Epoch: 0196\n",
      "Epoch: 0197\n",
      "Epoch: 0198\n",
      "Epoch: 0199\n",
      "Epoch: 0200\n",
      "RBM training Completed !\n",
      "Iter 0, Minibatch Loss= 9.649413, Training Accuracy= 0.08594\n",
      "Iter 10, Minibatch Loss= 2.962997, Training Accuracy= 0.35547\n",
      "Iter 20, Minibatch Loss= 1.231283, Training Accuracy= 0.69531\n",
      "Iter 30, Minibatch Loss= 1.178100, Training Accuracy= 0.70312\n",
      "Iter 40, Minibatch Loss= 0.913115, Training Accuracy= 0.79688\n",
      "Iter 50, Minibatch Loss= 0.905112, Training Accuracy= 0.78516\n",
      "Iter 60, Minibatch Loss= 0.821046, Training Accuracy= 0.79688\n",
      "Iter 70, Minibatch Loss= 0.714523, Training Accuracy= 0.80469\n",
      "Iter 80, Minibatch Loss= 0.792848, Training Accuracy= 0.80859\n",
      "Iter 90, Minibatch Loss= 0.704960, Training Accuracy= 0.82422\n",
      "Iter 100, Minibatch Loss= 0.664267, Training Accuracy= 0.81641\n",
      "Iter 110, Minibatch Loss= 0.678846, Training Accuracy= 0.82031\n",
      "Iter 120, Minibatch Loss= 0.320242, Training Accuracy= 0.91016\n",
      "Iter 130, Minibatch Loss= 0.531790, Training Accuracy= 0.85547\n",
      "Iter 140, Minibatch Loss= 0.455287, Training Accuracy= 0.86719\n",
      "Iter 150, Minibatch Loss= 0.663083, Training Accuracy= 0.83594\n",
      "Iter 160, Minibatch Loss= 0.642975, Training Accuracy= 0.83984\n",
      "Iter 170, Minibatch Loss= 0.431696, Training Accuracy= 0.87109\n",
      "Iter 180, Minibatch Loss= 0.424991, Training Accuracy= 0.90234\n",
      "Iter 190, Minibatch Loss= 0.571112, Training Accuracy= 0.85547\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8759\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리를 불러들임 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# MNIST 파일 읽어들임\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "# 학습관련 매개변수 설정\n",
    "n_input      = 784\n",
    "n_hidden1    = 500\n",
    "n_hidden2    = 256\n",
    "display_step = 1\n",
    "n_epoch = 200 \n",
    "batch_size = 256 \n",
    "lr_rbm = tf.constant(0.001, tf.float32)\n",
    "lr_class = tf.constant(0.01, tf.float32)\n",
    "n_class = 10\n",
    "n_iter = 200\n",
    "\n",
    "# 입력 및 출력을 정의함\n",
    "x  = tf.placeholder(tf.float32, [None, n_input], name=\"x\") \n",
    "y  = tf.placeholder(tf.float32, [None,10], name=\"y\") \n",
    "\n",
    "# 첫 번째 은닉층 관련 가중치 및 편향을 정의함\n",
    "W1  = tf.Variable(tf.random_normal([n_input, n_hidden1], 0.01), name=\"W1\") \n",
    "b1_h = tf.Variable(tf.zeros([1, n_hidden1],  tf.float32, name=\"b1_h\")) \n",
    "b1_i = tf.Variable(tf.zeros([1, n_input],  tf.float32, name=\"b1_i\")) \n",
    "\n",
    "# 두 번째 은닉층 관련 가중치 및 편향을 정의함\n",
    "W2  = tf.Variable(tf.random_normal([n_hidden1, n_hidden2], 0.01), name=\"W2\") \n",
    "b2_h = tf.Variable(tf.zeros([1, n_hidden2],  tf.float32, name=\"b2_h\")) \n",
    "b2_i = tf.Variable(tf.zeros([1, n_hidden1],  tf.float32, name=\"b2_i\")) \n",
    "\n",
    "# 라벨층 관련 가중치 및 편향을 정의함\n",
    "W_c = tf.Variable(tf.random_normal([n_hidden2,n_class], 0.01), name=\"W_c\") \n",
    "b_c = tf.Variable(tf.zeros([1, n_class],  tf.float32, name=\"b_c\")) \n",
    "\n",
    "# 확률을 이산 상태, 즉 0과 1로 변환함 \n",
    "def binary(prob):\n",
    "    return tf.floor(prob + tf.random_uniform(tf.shape(prob), 0, 1))\n",
    "\n",
    "# Gibbs 표본추출 단계\n",
    "def cd_step(x_k,W,b_h,b_i):\n",
    "    h_k = binary(tf.sigmoid(tf.matmul(x_k, W) + b_h)) \n",
    "    x_k = binary(tf.sigmoid(tf.matmul(h_k, tf.transpose(W)) + b_i))\n",
    "    return x_k\n",
    "\n",
    "# 표본추출 단계 실행     \n",
    "def cd_gibbs(k,x_k,W,b_h,b_i):\n",
    "    for i in range(k):\n",
    "        x_out = cd_step(x_k,W,b_h,b_i) \n",
    "    # k 반복 후에 깁스 표본을 반환함\n",
    "    return x_out\n",
    "\n",
    "# 2개의 은닉층을 갖는 DBN에 대한 CD-2 알고리즘\n",
    "# 1. 현재 입력값을 기반으로 깁스 표본추출을 통해 새로운 입력값 x_s를 구함\n",
    "# 2. 새로운 x_s를 기반으로 새로운 은닉노드 값 h_s를 구함    \n",
    "x_s = cd_gibbs(2,x,W1,b1_h,b1_i) \n",
    "act_h1_s = binary(tf.sigmoid(tf.matmul(x_s, W1) + b1_h)) \n",
    "h1_s = cd_gibbs(2,act_h1_s,W2,b2_h,b2_i) \n",
    "act_h2_s = binary(tf.sigmoid(tf.matmul(h1_s, W2) + b2_h)) \n",
    "\n",
    "# 입력값이 주어질 때 은닉노드 값 h를 구함\n",
    "act_h1 = tf.sigmoid(tf.matmul(x, W1) + b1_h) \n",
    "act_h2 = tf.sigmoid(tf.matmul(act_h1_s, W2) + b2_h) \n",
    "\n",
    "# 경사 하강법을 이용한 가중치 및 편향 업데이트 \n",
    "size_batch = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "\n",
    "W1_add  = tf.multiply(lr_rbm/size_batch, tf.subtract(tf.matmul(tf.transpose(x), \\\n",
    "          act_h1), tf.matmul(tf.transpose(x_s), act_h1_s)))\n",
    "b1_i_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(x, x_s), \\\n",
    "           0, True))\n",
    "b1_h_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(act_h1, act_h1_s), \\\n",
    "           0, True))\n",
    "\n",
    "W2_add  = tf.multiply(lr_rbm/size_batch, tf.subtract(tf.matmul(tf.transpose(act_h1_s), \\\n",
    "          act_h2), tf.matmul(tf.transpose(h1_s), act_h2_s)))\n",
    "b2_i_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(act_h1_s, h1_s), \\\n",
    "           0, True))\n",
    "b2_h_add = tf.multiply(lr_rbm/size_batch, tf.reduce_sum(tf.subtract(act_h2, act_h2_s), \\\n",
    "        0, True))\n",
    "\n",
    "updt = [W1.assign_add(W1_add), b1_i.assign_add(b1_i_add), b1_h.assign_add(b1_h_add),\\\n",
    "        W2.assign_add(W2_add), b2_i.assign_add(b2_i_add), b2_h.assign_add(b2_h_add)]\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "# 소프트맥스 층을 추가한 분류용-DBN 을 위한 연산과정\n",
    "#------------------------------------------------------------- \n",
    "\n",
    "logits = tf.matmul(act_h2,W_c) + b_c\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr_class).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "#-------------------------------------------------------------                   \n",
    "# RBM을 쌓아 올려가며 DBN을 학습하는 텐서플로 그래프 실행   \n",
    "#-------------------------------------------------------------\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the variables of the Model\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    n_batch = int(mnist.train.num_examples/batch_size)\n",
    "    # Start the training \n",
    "    for epoch in range(n_epoch):\n",
    "        # Loop over all batches\n",
    "        for i in range(n_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Run the weight update \n",
    "            batch_xs = (batch_xs > 0)*1\n",
    "            _ = sess.run([updt], feed_dict={x:batch_xs})\n",
    "            \n",
    "        # Display the running step \n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1))\n",
    "                  \n",
    "    print(\"RBM training Completed !\")\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "# 소프트맥스 층을 추가한 분류용-DBN 학습 및 예측\n",
    "#--------------------------------------------------------------\n",
    "    for i in range(n_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # 최적화 과정 실행\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if i % 10 == 0:\n",
    "            # MINIST 훈련용 이미지의 배치에 대한 손실과 정확도를 계산\n",
    "            tr_loss, tr_acc = sess.run([cost, accuracy], \n",
    "            \t                    feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(i) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(tr_loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(tr_acc))\n",
    "        \n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # MINIST 검정용 이미지에 대한 정확도 계산 \n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                      y: mnist.test.labels}))\n",
    "\n",
    "    sess.close()\n",
    "    # Testing Accuracy: 0.8769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
