{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch: 0010\n",
      "Epoch: 0020\n",
      "Epoch: 0030\n",
      "Epoch: 0040\n",
      "Epoch: 0050\n",
      "Epoch: 0060\n",
      "Epoch: 0070\n",
      "Epoch: 0080\n",
      "Epoch: 0090\n",
      "Epoch: 0100\n",
      "Epoch: 0110\n",
      "Epoch: 0120\n",
      "Epoch: 0130\n",
      "Epoch: 0140\n",
      "Epoch: 0150\n",
      "Epoch: 0160\n",
      "Epoch: 0170\n",
      "Epoch: 0180\n",
      "Epoch: 0190\n",
      "Epoch: 0200\n",
      "Discriminative RBM training Completed !\n",
      "Training Accuracy: 0.9142857\n",
      "Test Accuracy: 0.9111111\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "예제 3-3: 분류용-RBM을 MNIST 데이터에 적용\n",
    "\"\"\"\n",
    "\n",
    "## 필요한 라이브러리를 불러들임 \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "## iris 파일 읽어들여 입력 데이터와 출력 데이터로 분리\n",
    "iris = datasets.load_iris()\n",
    "irisX = iris.data\n",
    "irisY = iris.target\n",
    "# 원-핫 벡터 생성\n",
    "irisY = pd.get_dummies(irisY)  \n",
    "irisY = np.array(irisY)  \n",
    "\n",
    "## 이진 입력 RBM을 위한 입력 데이터의 정규화 \n",
    "minmax = np.amin(irisX, 0), np.amax(irisX, 0)\n",
    "no_irisX = (irisX-minmax[0])/(minmax[1]-minmax[0])\n",
    "\n",
    "## 훈련 데이터와 검정 데이터를 7:3 비율로 분리 \n",
    "np.random.seed(2019)\n",
    "ind1 = np.random.permutation(50)\n",
    "p_ind2 = np.arange(50,100)\n",
    "ind2 = np.random.permutation(p_ind2) \n",
    "p_ind3 = np.arange(100,150)\n",
    "ind3 = np.random.permutation(p_ind3) \n",
    "\n",
    "tr_ind1 = ind1[:35]\n",
    "tr_ind2 = ind2[:35]\n",
    "tr_ind3 = ind3[:35]\n",
    "tr_ind = np.concatenate((tr_ind1,tr_ind2,tr_ind3),axis=0)\n",
    "te_ind1 = ind1[35:]\n",
    "te_ind2 = ind2[35:]\n",
    "te_ind3 = ind3[35:]\n",
    "te_ind = np.concatenate((te_ind1,te_ind2,te_ind3),axis=0)\n",
    "\n",
    "trX = no_irisX[tr_ind]\n",
    "teX = no_irisX[te_ind]\n",
    "trY = irisY[tr_ind]\n",
    "teY = irisY[te_ind]\n",
    "\n",
    "## 학습관련 매개변수 설정 \n",
    "n_input     = 4\n",
    "n_hidden    = 20\n",
    "display_step = 10\n",
    "num_epochs = 200 \n",
    "batch_size = 5 \n",
    "lr         = tf.constant(0.01, tf.float32)\n",
    "n_class = 3\n",
    "\n",
    "## 입력, 가중치 및 편향을 정의함\n",
    "x  = tf.placeholder(tf.float32, [None, n_input], name=\"x\") \n",
    "y  = tf.placeholder(tf.float32, [None,n_class], name=\"y\") \n",
    "W_xh  = tf.Variable(tf.random_normal([n_input, n_hidden], 0.01), name=\"W_xh\") \n",
    "W_hy = tf.Variable(tf.random_normal([n_hidden,n_class], 0.01), name=\"W_hy\")\n",
    "b_i = tf.Variable(tf.zeros([1, n_input],  tf.float32, name=\"b_i\")) \n",
    "b_h = tf.Variable(tf.zeros([1, n_hidden],  tf.float32, name=\"b_h\"))\n",
    "b_y = tf.Variable(tf.zeros([1, n_class],  tf.float32, name=\"b_y\")) \n",
    "\n",
    "## 확률을 이산 상태, 즉 0과 1로 변환함  \n",
    "def binary(probs):\n",
    "    return tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))\n",
    "          \n",
    "## Gibbs 표본추출 단계\n",
    "def gibbs_step(x_k,y_k):\n",
    "        h_k = binary(tf.sigmoid(tf.matmul(x_k, W_xh) + tf.matmul(y_k, tf.transpose(W_hy)) + b_h)) \n",
    "        x_k = binary(tf.sigmoid(tf.matmul(h_k, tf.transpose(W_xh)) + b_i))\n",
    "        y_k = tf.nn.softmax(tf.matmul(h_k, W_hy) + b_y)\n",
    "        return x_k,y_k\n",
    "\n",
    "## 표본추출 단계 실행    \n",
    "def gibbs_sample(k,x_k,y_k):\n",
    "    for i in range(k):\n",
    "        x_out,y_out = gibbs_step(x_k,y_k) \n",
    "## k 반복 후에 깁스 표본을 반환함\n",
    "    return x_out,y_out\n",
    "\n",
    "## CD-2 알고리즘\n",
    "# 현재 입력값 및 출력값을 기반으로 깁스 표본추출을 통해 새로운 입력값 x_s, y_s를 구함\n",
    "x_s,y_s = gibbs_sample(2,x,y)\n",
    "# 새로운 x_s, y_s를 기반으로 새로운 은닉노드 값 act_h_s를 구함       \n",
    "act_h_s = tf.sigmoid(tf.matmul(x_s, W_xh) + tf.matmul(y_s, tf.transpose(W_hy)) + b_h)  \n",
    "# 입력값 및 출력값이 주어질 때 은닉노드 값 act_h를 구함\n",
    "act_h = tf.sigmoid(tf.matmul(x, W_xh) + tf.matmul(y, tf.transpose(W_hy)) + b_h) \n",
    "# 은닉노드 값이 주어질 때 입력값을 추출함\n",
    "_x = tf.sigmoid(tf.matmul(act_h, tf.transpose(W_xh)) + b_i)\n",
    "\n",
    "## 경사 하강법을 이용한 분류용-RBM의 가중치 및 편향 업데이트 \n",
    "W_xh_add = tf.multiply(lr/batch_size, tf.subtract(tf.matmul(tf.transpose(x), act_h), \\\n",
    "\t       tf.matmul(tf.transpose(x_s), act_h_s)))\n",
    "W_hy_add = tf.multiply(lr/batch_size, tf.subtract(tf.matmul(tf.transpose(act_h), y), \\\n",
    "\t       tf.matmul(tf.transpose(act_h_s), y_s)))\n",
    "bi_add = tf.multiply(lr/batch_size, tf.reduce_sum(tf.subtract(x, x_s), 0, True))\n",
    "bh_add = tf.multiply(lr/batch_size, tf.reduce_sum(tf.subtract(act_h, act_h_s), 0, True))\n",
    "by_add = tf.multiply(lr/batch_size, tf.reduce_sum(tf.subtract(y, y_s), 0, True))\n",
    "updt = [W_xh.assign_add(W_xh_add), W_hy.assign_add(W_hy_add), b_i.assign_add(bi_add),\\\n",
    "        b_h.assign_add(bh_add), b_y.assign_add(by_add)]\n",
    "\n",
    "\n",
    "# 텐서플로 그래프 실행\n",
    "with tf.Session() as sess:\n",
    "    # 모형의 변수를 초기화하기\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # 학습 시작  \n",
    "    for epoch in range(num_epochs):\n",
    "        ind2 = np.random.permutation(len(trX))\n",
    "        num_batch = int(len(trX)/batch_size)\n",
    "        # 모든 미니배치에 대해 반복함\n",
    "        for i in range(num_batch):\n",
    "            batch_xs = trX[ind2[i*batch_size:(i+1)*batch_size]]\n",
    "            batch_ys = trY[ind2[i*batch_size:(i+1)*batch_size]]\n",
    "            # 가중치 업데이터 실행 \n",
    "#            batch_x = (batch_x > 0)*1\n",
    "            _ = sess.run([updt], feed_dict={x:batch_xs, y:batch_ys})\n",
    "            \n",
    "        # 매 에포크마다 실행 단계 보여주기 \n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+10))\n",
    "\n",
    "    print(\"Discriminative RBM training Completed !\")\n",
    "   \n",
    "    ## 훈련 데이터에 대한 분류용-RBM의 정확도 계산\n",
    "    tr_lab1 = np.zeros((len(trX),n_class)); tr_lab1[:,0] = 1\n",
    "    tr_lab2 = np.zeros((len(trX),n_class)); tr_lab2[:,1] = 1\n",
    "    tr_lab3 = np.zeros((len(trX),n_class)); tr_lab3[:,2] = 1\n",
    "    \n",
    "    tr_f1_xl = tf.reduce_sum(tf.nn.softplus(tf.matmul(tf.cast(trX,tf.float32), W_xh) \\\n",
    "    \t       + tf.matmul(tf.cast(tr_lab1,tf.float32), tf.transpose(W_hy)) + b_h),1)\n",
    "    tr_f2_xl = tf.reduce_sum(tf.nn.softplus(tf.matmul(tf.cast(trX,tf.float32), W_xh) \\\n",
    "    \t       + tf.matmul(tf.cast(tr_lab2,tf.float32), tf.transpose(W_hy)) + b_h),1)\n",
    "    tr_f3_xl = tf.reduce_sum(tf.nn.softplus(tf.matmul(tf.cast(trX,tf.float32), W_xh) \\\n",
    "    \t       + tf.matmul(tf.cast(tr_lab3,tf.float32), tf.transpose(W_hy)) + b_h),1)\n",
    "\n",
    "    tr_f_xl = b_y + tf.transpose([tr_f1_xl,tr_f2_xl,tr_f3_xl])\n",
    "    tr_y_hat = tf.nn.softmax(tr_f_xl)\n",
    "        \n",
    "    tr_correct_pred = tf.equal(tf.argmax(tr_y_hat,1), tf.argmax(trY, 1))\n",
    "    tr_accuracy = tf.reduce_mean(tf.cast(tr_correct_pred, tf.float32))\n",
    "\n",
    "    print(\"Training Accuracy:\", sess.run(tr_accuracy))\n",
    "    \n",
    "    ## 검정 데이터에 대한 분류용-RBM의 정확도 계산 \n",
    "    te_lab1 = np.zeros((len(teX),n_class)); te_lab1[:,0] = 1\n",
    "    te_lab2 = np.zeros((len(teX),n_class)); te_lab2[:,1] = 1\n",
    "    te_lab3 = np.zeros((len(teX),n_class)); te_lab3[:,2] = 1\n",
    "    \n",
    "    te_f1_xl = tf.reduce_sum(tf.nn.softplus(tf.matmul(tf.cast(teX,tf.float32), W_xh) \\\n",
    "    \t       + tf.matmul(tf.cast(te_lab1,tf.float32), tf.transpose(W_hy)) + b_h),1)\n",
    "    te_f2_xl = tf.reduce_sum(tf.nn.softplus(tf.matmul(tf.cast(teX,tf.float32), W_xh) \\\n",
    "    \t       + tf.matmul(tf.cast(te_lab2,tf.float32), tf.transpose(W_hy)) + b_h),1)\n",
    "    te_f3_xl = tf.reduce_sum(tf.nn.softplus(tf.matmul(tf.cast(teX,tf.float32), W_xh) \\\n",
    "    \t       + tf.matmul(tf.cast(te_lab3,tf.float32), tf.transpose(W_hy)) + b_h),1)\n",
    "\n",
    "    te_f_xl = b_y + tf.transpose([te_f1_xl,te_f2_xl,te_f3_xl])\n",
    "    te_y_hat = tf.nn.softmax(te_f_xl)\n",
    "    \n",
    "    te_correct_pred = tf.equal(tf.argmax(te_y_hat,1), tf.argmax(teY, 1))\n",
    "    te_accuracy = tf.reduce_mean(tf.cast(te_correct_pred, tf.float32))\n",
    "\n",
    "    print(\"Test Accuracy:\", sess.run(te_accuracy))\n",
    "    \n",
    "    sess.close()\n",
    "\n",
    "\n",
    "#Training Accuracy: 0.9238095\n",
    "#Test Accuracy: 0.95555556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
