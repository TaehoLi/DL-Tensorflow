{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-774565ee34a8>:68: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-1-774565ee34a8>:76: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-774565ee34a8>:90: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-1-774565ee34a8>:100: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\my\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "epoch=0, loss=3.2930\n",
      "epoch=1, loss=2.8827\n",
      "epoch=2, loss=2.6449\n",
      "epoch=3, loss=2.4896\n",
      "epoch=4, loss=2.3897\n",
      "epoch=5, loss=2.3136\n",
      "epoch=6, loss=2.2570\n",
      "epoch=7, loss=2.2119\n",
      "epoch=8, loss=2.1781\n",
      "epoch=9, loss=2.1532\n",
      "epoch=10, loss=2.1276\n",
      "epoch=11, loss=2.1087\n",
      "epoch=12, loss=2.0989\n",
      "epoch=13, loss=2.0815\n",
      "epoch=14, loss=2.0688\n",
      "epoch=15, loss=2.0654\n",
      "epoch=16, loss=2.0534\n",
      "epoch=17, loss=2.0418\n",
      "epoch=18, loss=2.0278\n",
      "epoch=19, loss=2.0202\n",
      "epoch=20, loss=2.0042\n",
      "epoch=21, loss=1.9931\n",
      "epoch=22, loss=1.9884\n",
      "epoch=23, loss=1.9774\n",
      "epoch=24, loss=1.9706\n",
      "epoch=25, loss=1.9654\n",
      "epoch=26, loss=1.9670\n",
      "epoch=27, loss=1.9662\n",
      "epoch=28, loss=1.9592\n",
      "epoch=29, loss=1.9565\n",
      "epoch=30, loss=1.9545\n",
      "epoch=31, loss=1.9468\n",
      "epoch=32, loss=1.9470\n",
      "epoch=33, loss=1.9409\n",
      "epoch=34, loss=1.9350\n",
      "epoch=35, loss=1.9372\n",
      "epoch=36, loss=1.9363\n",
      "epoch=37, loss=1.9340\n",
      "epoch=38, loss=1.9383\n",
      "epoch=39, loss=1.9350\n",
      "epoch=40, loss=1.9296\n",
      "epoch=41, loss=1.9289\n",
      "epoch=42, loss=1.9289\n",
      "epoch=43, loss=1.9262\n",
      "epoch=44, loss=1.9258\n",
      "epoch=45, loss=1.9269\n",
      "epoch=46, loss=1.9228\n",
      "epoch=47, loss=1.9162\n",
      "epoch=48, loss=1.9128\n",
      "epoch=49, loss=1.9070\n",
      "t;,'s! te not poustent.\n",
      "\n",
      "VRRGIUIUS:\n",
      "The var lritodse tibe; Cohe\n",
      "In stosence, not exted extost waald, in our staok owcenterss baen use Rome.\n",
      "\n",
      "SICINIUS:\n",
      "I are spaiusiles powed onkan you.\n",
      "\n",
      "VARGIUS:\n",
      "Hon od\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# 그래프 리셋\n",
    "tf.reset_default_graph()\n",
    "# 재현성을 위해 시드 지정\n",
    "tf.set_random_seed(1)\n",
    "#-------------------------------------------------\n",
    "# 데이터 불러오기\n",
    "#-------------------------------------------------\n",
    "raw_data = open('input.txt', 'r').read() # input.txt 예제 데이터\n",
    "# 계산 시간 때문에 일부만 사용\n",
    "raw_data = raw_data[:200000]\n",
    "# 전체 문자 수\n",
    "n_samples = len(raw_data)\n",
    "# 고유한 문자\n",
    "unique_chars = list(set(raw_data));\n",
    "# 문자를 정수로 변환하는 딕셔너리\n",
    "char_to_int = { ch:i for i,ch in enumerate(unique_chars) }\n",
    "# 정수를 문자로 변환하는 딕셔너리\n",
    "int_to_char = { i:ch for i,ch in enumerate(unique_chars) }\n",
    "# 고유한 문자 수\n",
    "n_unique_chars = len(unique_chars)\n",
    "\n",
    "# 입력층의 노드 수(입력 크기) = 원-핫 벡터 크기\n",
    "input_dim = n_unique_chars\n",
    "# 출력층의 노드 수 = 고유한 문자 수\n",
    "num_classes = n_unique_chars\n",
    "#-------------------------------------------\n",
    "# 매개변수 설정\n",
    "#-------------------------------------------\n",
    "# 은닉층 크기: 32\n",
    "# 미니배치 크기: 100\n",
    "# 반복 수: 200\n",
    "# 학습률: 0.1\n",
    "# Gradient Clipping에 사용할 임곗값: 2\n",
    "batch_size = 100\n",
    "seq_len = 50 # 한 번에 입력되는 서열 길이\n",
    "hidden_size = 32 # 은닉층 노드 수\n",
    "learning_rate = 0.05\n",
    "grad_clip = 5 # Gradient Clipping에 사용할 임곗값\n",
    "nepochs = 50 # 반복 수\n",
    "# 배치 개수\n",
    "num_batches = int(len(raw_data)/(batch_size * seq_len))\n",
    "# 데이터 - (배치 크기*배치 개수)\n",
    "data = raw_data[:num_batches*batch_size*seq_len]\n",
    "# 데이터를 정수로 변환\n",
    "data = np.array([char_to_int[n] for n in data]);\n",
    "# 입력과 목표 데이터 설정\n",
    "xdata = data; ydata = np.copy(data)\n",
    "ydata[:-1] = xdata[1:]; ydata[-1] = xdata[0]\n",
    "# 배치 개수*배치 크기*서열 길이 = (?, 100, 32)\n",
    "x_batches = np.split(xdata.reshape(batch_size, -1),num_batches, 1)\n",
    "x_batches = np.asarray(x_batches)\n",
    "#x_batches.shape\n",
    "\n",
    "y_batches = np.split(ydata.reshape(batch_size, -1), num_batches, 1)\n",
    "y_batches = np.asarray(y_batches)\n",
    "# 입력 데이터 플레이스 홀더 설정([batch_size, seq_len])\n",
    "X = tf.placeholder(tf.int32, shape = [None, None])\n",
    "# 목표 데이터 플레이스 홀더 설정([batch_size*seq_len, n_unique_chars])\n",
    "Y = tf.placeholder(tf.int32, shape = [None, None])\n",
    "state_batch_size = tf.placeholder(tf.int32, shape=[]) # Training: 100, Sampling: 1\n",
    "# LSTM으로 cell을 정의한다.\n",
    "cell_1 = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "# 드롭아웃 사용\n",
    "cell_1 = tf.nn.rnn_cell.DropoutWrapper(cell_1, output_keep_prob=0.8)\n",
    "# 은닉층 추가\n",
    "cell_2 = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "# 은닉층 추가\n",
    "cell_3 = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "# 은닉층이 3개인 LSTM\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([cell_1, cell_2, cell_3])\n",
    "# 각 문자를 원-핫 코딩이 아닌 hidden_size 크기의 숫자 벡터로 변환\n",
    "# 즉 n_unique_chars -> hidden_size\n",
    "embedding = tf.Variable(tf.random_normal(shape=[n_unique_chars, hidden_size]),\n",
    "dtype = tf.float32)\n",
    "X_one_hot = tf.nn.embedding_lookup(embedding, X)\n",
    "# X_one_hot = tf.one_hot(X, n_unique_chars)\n",
    "# 초기 state 값을 0으로 초기화\n",
    "initial_state = cell.zero_state(state_batch_size, tf.float32)\n",
    "# 학습을 위한 tf.nn.dynamic_rnn을 선언\n",
    "# 서열의 길이가 일정하기 때문에 static를 사용해도 되지만 dynamic이 메모리 관점에서\n",
    "# 장점이 있음\n",
    "# outputs의 형태는 [batch_size, seq_len, hidden_size]\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, X_one_hot, initial_state = initial_state,\n",
    "dtype = tf.float32)\n",
    "\n",
    "# ouputs을 [batch_size * seq_len, hidden_size]] 형태로 바꾸기\n",
    "output = tf.reshape(outputs, [-1, hidden_size])\n",
    "# 은닉층의 값으로 출력을 만듦\n",
    "# 은닉층의 결과를 완전 연결층을 통하여 분류\n",
    "model = tf.contrib.layers.fully_connected(inputs = output, num_outputs = num_classes,\n",
    "activation_fn = None)\n",
    "prediction = tf.nn.softmax(model)\n",
    "# 손실함수\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels = Y))\n",
    "# 변수 선언\n",
    "tvars = tf.trainable_variables()\n",
    "# 기울기 클리핑\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "# 옵티마이저\n",
    "optim = tf.train.AdamOptimizer(learning_rate)\n",
    "# 기울기 적용 (optim.AdamOptimizer().minimize() 대신 사용됨)\n",
    "train_step = optim.apply_gradients(zip(grads, tvars))\n",
    "#------------------------------------------------\n",
    "# 텐서플로 그래프 생성 및 학습 \n",
    "#------------------------------------------------\n",
    "sess=tf.Session();  \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "losses=[]\n",
    "for epoch in range(nepochs):   \n",
    "    bat_loss=[]\n",
    "    state = sess.run(initial_state, feed_dict={state_batch_size : batch_size})\n",
    "    for j in range(x_batches.shape[0]):\n",
    "        state = sess.run(initial_state, feed_dict={state_batch_size : batch_size})\n",
    "        xx = x_batches[j,:,:]\n",
    "        y0 = y_batches[j,:,:]\n",
    "        y0 = np_utils.to_categorical(y0, n_unique_chars)  # [batch_size, seq_len, n_unique_chars]\n",
    "        \n",
    "        yy = np.reshape(y0, [-1, n_unique_chars])       # [batch_size * seq_len, n_unique_chars]\n",
    "        yy=np.array(yy, dtype=np.int32)\n",
    "        # feed-dict에 사용할 값들과 LSTM 초기 cell state(feed_dict[c])값과\n",
    "        # hidden layer 출력값(feed_dict[h])을 지정\n",
    "        feed_dict = {X:xx, Y:yy, state_batch_size:batch_size}\n",
    "        for i, (c, h) in enumerate(initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        # 한스텝 학습을 진행.\n",
    "        _, loss, state = sess.run([train_step, cost, final_state], feed_dict=feed_dict)  \n",
    "        bat_loss.append(loss)\n",
    "    mean_bat_loss=np.array(bat_loss).mean()        \n",
    "    \n",
    "    if ((epoch%1==0)):\n",
    "            print('epoch={}, loss={:.4f}' .format(epoch, mean_bat_loss))\n",
    "    losses.append(mean_bat_loss)  \n",
    "        #print(bat_loss)\n",
    "\n",
    "# 훈련과정의 loss 그림  \n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses)\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('Losses')\n",
    "\n",
    "\n",
    "# 생성할 문장의 길이\n",
    "generated_text_len = 200  \n",
    "\n",
    "# 시작 문자를 't'로 지정.\n",
    "generated_text = 't' \n",
    "# RNN의 최초 state값을 0으로 초기화.\n",
    "state = sess.run(cell.zero_state(1, tf.float32))\n",
    "\n",
    "for n in range(generated_text_len):\n",
    "    \n",
    "    if len(generated_text)> seq_len:\n",
    "        gen_text_input=generated_text[1:]\n",
    "    else:   \n",
    "        gen_text_input=generated_text\n",
    "        \n",
    "    gen_text_input=np.array([char_to_int[x] for x in generated_text]).reshape(-1,len(generated_text))\n",
    "    \n",
    "    [probs_result, state] = sess.run([prediction,final_state], feed_dict={X: gen_text_input, state_batch_size:1,\n",
    "                                                                          initial_state:state})   \n",
    "    p = np.squeeze(probs_result)[-1] \n",
    "    # 다음 문자를 예측할때 확률을 이용한 random sampling을 사용\n",
    "    sample=int(np.searchsorted(np.cumsum(p), np.random.rand(1)*np.sum(p)))  \n",
    "    pred = int_to_char[sample]\n",
    "    # 생성된 문자열에 현재 스텝에서 예측한 문자추가. \n",
    "    generated_text += pred\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
